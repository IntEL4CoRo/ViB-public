---
title: "Computer Vision"
date: 2025-02-01T00:00:00-00:00
subtitle: ""
tags: ["Fame", "Innovation"]
dropCap: false
displayInMenu: false
displayInList: true
draft: false
---

Identifying (classification + segmentation mask) and estimating the 6D pose (orientation and position) of specific objects from a cluttered scene is a fundamental problem in computer vision and has many use cases in the industry, e.g. locate moving objects on a conveyor belt to pick them up via robot arms.\
Futhermore, it can be used to create a digitial twin/belief state of a video showing a human performing some task, e.g. making breakfast, via performing a framewise video analysis and using the 3D CAD-models of the idendified object instances and their estimated poses.
Through further analysis in a physics engine, e.g. MuJoCo, a scene graph can be created and a deeper understanding of the performed actions (movement/manipulation + object acted on) and their context among each other can be achieved.

However, many of the classical identifying and pose estimation methods struggle often, as they depend on additional depth information besides normal color/rgb images, which need specific sensors, and of being object-specific. Thus, these methods can only handle the specific objects seen during training, therefore, when they are applied to a novel/unknown object, a complicated and time consuming onboarding process/model retraining is required.\
This limits their practicality in learing performed actions from internet videos, as these don't have the needed depth together with the presence of novel objects.
But, in the last few years, much progress was made in the direction of developing novel zero-shot frameworks that can perform object identification and pose estimation for novel objects defined/specified only by their CAD-models at run time (no retraining required).

Internally these modern approaches usually first leverage the object and/or task agnostic generalizion power of recent (image) foundation models to take care of sub tasks, like e.g. segmentation mask generation (via SAM/GroundingSAM) and image embeddings (via DINOv2).
Afterwards, they apply more traditional algorithms, for e.g. Perspective-n-Point matching, and deep learning architectures (e.g. CNN or Transformer).

Moreover, with the constant increasing number of public available high qulality CAD-model datasets, e.g. "Google Scanned Objects" with more than 1000 household items or "ShapeNetCore" with 513000 models, many common household items seen in videos can be associated with a fitting CAD-model.\
Otherwise, progress in the fields of structure from motion (e.g. HLoc), model diffusion (like image diffusion for 3d models, e.g. Wonder3D) and text-to-texture (image-) diffusion to retexture/reuse an existing (categorical) basic mesh can be used to create  a certain object based on multiple video frames, if for no fitting CAD-model is available.

## Connection with FAME

### Main Goal

- Extracted object/ human actor information from the scene are used in
  - Scene understanding via mental/physics-based simulation
  - Basic parameters for action descriptor to (partly) replicate the observed scene action


### Sub Tasks

- Instance segmentation for only relevant objects
- Estimate the 3D pose of the detected objects
- Extract human actors


### Challenges

- Only (cheap) unknown cameras
  - Color/RGB images only (no depth)
  - Missing camera intrinsic parameter
  - E.g. YouTube, WikiHow or live demonstration
- Should work on single images/frames and sequences
- Scene can contain unknown/unseen objects
  - No re-training should be needed


### Relevancy

- Object detection and pose estimation are a basic problem in computer vision
  - Not only for learning, but robots also need these while executing an action
  - Many Industrial use cases
    - E.g. Finding and grabbing objects from an conveyor belt


### Solutions for the object perception

- Use current Image-Foundation-Models
  - Pre-trained with many generic objects/images


### Future

- ***Replace*** current pipeline components with ***self developed/better*** ones
- Use 3D-models from knowledge base to represent the relevant objects
  - Also contains information for physics reasoning
  - Create missing objects e.g. with structure-from-motion
- Estimate missing camera intrinsic paramete
- ***Extract*** human actors information


## Example Images

{{< gallery >}}
  {{< my_figure link="img/example_img_1.jpg" caption="Original Image" >}}
  {{< my_figure link="img/example_img_2.jpg" caption="All object masks" alt="All masks generated by <a target='_blank' href='https://arxiv.org/abs/2401.14159'>GroundedSAM</a> with text prompt 'objects'." >}}
  {{< my_figure link="img/example_img_3.jpg" caption="Only specific objects" alt="<a target='_blank' href='https://nv-nguyen.github.io/cnos/'>CNOS</a> classifies the objects seen in the given masks and filters out unwanted ones. The image shows also the confidence and label">}}
  {{< my_figure link="img/example_img_4.jpg" caption="Coarse Pose Estimation" alt="<a target='_blank' href='https://nv-nguyen.github.io/gigapose/'>GigaPose</a> uses the 3D CAD model belonging to the object label and image mask to estimate the 6d object pose." >}}
  {{< my_figure link="img/example_img_5.jpg" caption="Refined Pose Estimation" alt="<a target='_blank' href='https://megapose6d.github.io/'>MegaPose</a> then refines the coarse pose estimation using the CAD model and image mask." >}}
{{< /gallery >}}


## Example Videos

- Instance Segmentation and 6D Pose Estimation of some <a target="_blank" href="https://www.ycbbenchmarks.com/">YCB</a> objects
  <figure class="video_container">
    <video width="100%" height="300" loop muted controls>
      <source src="vid/6dposes_grab.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  </figure>
<br>


## Navigate to FAME Parts

<div>
  For Detailed information on the differents parts of FAME click:<br>
  <div class="btn-group" style="width:100%">
    <a class="btn btn-primary" style="width:100%;" target="_blank" href="../"><b>Fame Main Page</b></a>
  </div>
  <div class="btn-group" style="width:100%">
    <a class="btn btn-success" style="width:33.3%;" target="_blank" href="../pose6d"><b>Computer Vision</b></a>
    <a class="btn btn-success" style="width:33.3%;" target="_blank" href="../action_segmentation"><b>Scene Understanding</b></a>
    <a class="btn btn-success" style="width:33.3%;" target="_blank" href="../prob_learning"><b>Probabilistic Models</b></a>
  </div>
  <div class="btn-group" style="width:100%">
    <a class="btn btn-success" style="width:33.3%;" target="_blank" href="../motion_control"><b>Geometric Control</b></a>
    <a class="btn btn-success" style="width:33.3%;" target="_blank" href="../geometric_learning"><b>Geometric Deep Learning</b></a>
    <a class="btn btn-success" style="width:33.3%;" target="_blank" href="../enviroment"><b>Environment Generation</b></a>
  </div>
</div>
<br>


# Authors

{{< author name="Max Gandyra" img="img/max.jpg" mail="mgandyra@uni-bremen.de" profile="https://ai.uni-bremen.de/team/max_gandyra" >}}
{{< author name="Alessandro Santonicola" img="img/alessandro.jpg" mail="ale_san@uni-bremen.de" profile="https://ai.uni-bremen.de/team/alessandro_santonicola" >}}

