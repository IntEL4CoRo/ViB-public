    ---
title: "Interactive Robot Task Learning Laboratory"
date: 2018-11-29T10:35:35-05:00
subtitle: ""
tags: ["Research"]
dropCap: false
displayInMenu: false
displayInList: true
draft: false
---

The Interactive Task Learning Lab delves into the forefront of robotics, exploring
advanced learning methodologies that empower robots with the ability to not only
perform specific tasks, but also to grasp the essence of the tasks themselves. This
includes understanding the goals, identifying potential unwanted side effects, and
determining suitable metrics for evaluating task performance. Beyond mere
execution, this approach necessitates that robots comprehend the physical laws
governing their environment, predict the outcomes of their actions, and align their
behavior with human expectations. Central to this lab’s research is the dynamic
interaction between robots and humans where the human acts as a teacher,
imparting knowledge about the conceptual framework of tasks, encompassing both
the underlying concepts and their interrelations. Robots are thus challenged to
recognize the limits of their knowledge and actively seek assistance, integrating and
acting upon advice in a manner that reflects an understanding of its purpose and
implications for modifying their actions. This innovative lab not only pushes the
boundaries of robot learning but also paves the way for more intuitive and
collaborative human-robot interactions.


For more information, you can
<a class="btn btn-success" target="_blank" href="https://ease-crc.github.io/itl/"><b>visit the webpage</b></a> of Interactive Task Learning to get a better idea on how a robot can learn from different teaching methodologies.

<div class="hidde-after-preview">
<a class="btn btn-primary" target="_blank" href="https://jupyter.intel4coro.de/user/abhijitvyas-boo-ng_with_giskard-jaorgsw8/lab/tree/RTC%3Anotebooks/bootstrapping_pouring_example.ipynb?token=n6ytk14ZRqWqOlQQRHmmUw">Run code</a>
<a class="btn btn-success" target="_blank" href="https://github.com/AbhijitVyas/pycram/tree/binder">Source Code</a>
</div>

<div class="hidde-after-preview">
  For Detailed information click
  <a class="btn btn-success" target="_blank" href="interactive-task-learning-by-natural-instruction-methods"><b>here!</b></a>
</div>

<!--more-->

<div class="main-well-flex-container" style="margin:20px;align-items: center;">

  <div style="flex:30%;">
      <img src="avyas2.jpg" style="clip-path: circle(35%);">
  </div>

  <div style="flex:70%;">
    <h3>Abhijit Vyas</h3>
    Tel:     +49 421 218 64026 <br>
    Fax:     +49 421 218 64047 <br>
    Mail:    <a href="mailto:avyas@cs.uni-bremen.de">avyas@cs.uni-bremen.de</a> <br>
    <a style="color:red" href="https://ai.uni-bremen.de/team/abhijit_vyas">
      <span style="font-size: 15px;">Profile</span>
    </a>
  </div>

</div>

![](ITL_framework.png)


Interactive Actions and/or Examples
---

<div>
<a class="btn btn-primary" target="_blank" href="https://jupyter.intel4coro.de/user/abhijitvyas-boo-ng_with_giskard-jaorgsw8/lab/tree/RTC%3Anotebooks/bootstrapping_pouring_example.ipynb?token=n6ytk14ZRqWqOlQQRHmmUw">Run code</a>
<a class="btn btn-success" target="_blank" href="https://github.com/AbhijitVyas/pycram/tree/binder">Source Code</a>
</div>


Description
---

ITL is an emerging A.I. challenge, defined as “any process by which an agent improves its performance on
some task through experience, when [that experience] consists of a series of sensing, effecting, and communicating interactions between (the agent), its world, and crucially other agents in the world(John Leird, Kevin A. Gluck).” An ITL setup is an apprentice-style learning approach where most aspects of the task can be explicitly taught by an instructor and the student can accumulate task-specific knowledge not only from interactions but also from past experiences to solve the novel task execution problem. We investigate the predominant natural interaction methods employed by humans to instruct each other, which include bootstrap task-specific instruction (”telling what to do”) and demonstration (”showing how to do it”).


Example Videos
---

- #### VR Human task demonstrations
  <figure class="video_container">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/Mp1-zXbcQ30?si=pYNNbpc6skGSYK_G" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
  </figure>

- #### VR Human task demonstrations as NEEM
  <figure class="video_container">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/GN5zkOYKxbY?si=UJi-87eeLVEfATo8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
  </figure>

- #### PR2 Pouring task demonstrations with PyCRAM
  <figure class="video_container">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/ofqdy3h2i24?si=Dm4L7n4x11zZxzTs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
  </figure>

